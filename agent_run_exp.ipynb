{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expriment Code [Using GPT API]:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Dataset 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import glob\n",
    "import gzip\n",
    "\n",
    "# load dataset function\n",
    "def load_split(dataset_dir, split):\n",
    "    # dataset_dir: root path of dataset\n",
    "    # split: either \"debug\",\"train\",\"val\" or \"test\"\n",
    "    # each dataset contains many episodes, which are all \".json.gz\" zip file.\n",
    "    split_paths = os.path.join(dataset_dir, split, \"episodes\", \"*.json.gz\")\n",
    "    split_paths = sorted(glob.glob(split_paths))\n",
    "\n",
    "    episode_list = []\n",
    "    dataset = {}\n",
    "\n",
    "    for split_path in split_paths:\n",
    "        # print(\"Loading: {path}\".format(path=split_path))\n",
    "\n",
    "        with gzip.GzipFile(split_path, \"r\") as f:\n",
    "            episodes = json.loads(f.read().decode(\"utf-8\"))\n",
    "\n",
    "            # Build a dictionary of the dataset indexed by scene, object_type\n",
    "            curr_scene = None\n",
    "            curr_object = None\n",
    "            points = []\n",
    "            scene_points = {}\n",
    "            for data_point in episodes:\n",
    "                if curr_object != data_point[\"object_type\"]:\n",
    "                    scene_points[curr_object] = points\n",
    "                    curr_object = data_point[\"object_type\"]\n",
    "                    points = []\n",
    "                if curr_scene != data_point[\"scene\"]:\n",
    "                    dataset[curr_scene] = scene_points\n",
    "                    curr_scene = data_point[\"scene\"]\n",
    "                    scene_points = {}\n",
    "\n",
    "                points.append(data_point)\n",
    "\n",
    "            episode_list += episodes\n",
    "    # reutrn type:\n",
    "    #    episode_list: list of init scene and target data, for setting and val.\n",
    "    #    dataset: dict of train path to nav object, for training.\n",
    "    return episode_list, dataset\n",
    "\n",
    "\n",
    "def load_dataset_episode(dataset_path, split_name, index):\n",
    "    test_episodes, dataset = load_split(\n",
    "        dataset_dir=dataset_path, split=split_name)\n",
    "    # print(\"episodes len: %d\" % len(test_episodes))\n",
    "    ep = test_episodes[index]\n",
    "\n",
    "    return ep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 手动执行 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_obj_nav_agent import Agent, OpType\n",
    "\n",
    "dataset_path = \"/home/nickbit/ai2thor/dataset/\"\n",
    "split_name = \"val\" # for some reason, \"test\" set are not provided by aithor 5.0.0\n",
    "\n",
    "agent = Agent(action_type=OpType.EIGHT_DIR_MOVE,\n",
    "              grid_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodes len: 1800\n"
     ]
    }
   ],
   "source": [
    "test_episodes, dataset = load_split(dataset_dir=dataset_path , split=split_name)\n",
    "print(\"episodes len: %d\" % len(test_episodes))\n",
    "\n",
    "# ep = test_episodes[101]\n",
    "# print(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_eps = []\n",
    "# sub_obj_types = []\n",
    "# for i in range(len(test_episodes)):\n",
    "#     ep = test_episodes[i]\n",
    "#     obj_type = ep['object_type']\n",
    "#     if obj_type not in sub_obj_types:\n",
    "#         sub_obj_types.append(obj_type)\n",
    "#         sub_eps.append(ep)\n",
    "#     else:\n",
    "#         if i % 50 == 1:\n",
    "#             sub_eps.append(ep)\n",
    "\n",
    "# print(len(sub_eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "ep = test_episodes[0]\n",
    "agent.mannual_do_first_step(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.mannual_do_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.draw_position_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = agent.make_controller_single_action(\"RotateRight\", 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event = agent.make_controller_single_action(\"MoveAhead\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test teleport\n",
    "import random\n",
    "\n",
    "event = agent.make_controller_single_action(\"RotateRight\", 0)\n",
    "objs = event.metadata['objects']\n",
    "random_index = random.randint(0, len(objs) - 1)\n",
    "to_tele_name = objs[random_index][\"objectType\"] \n",
    "print(to_tele_name)\n",
    "agent.make_controller_teleport_to_object(to_tele_name)\n",
    "event = agent.make_controller_single_action(\"RotateRight\", 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event = agent.make_controller_single_action(\"LookDown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_frame = event.depth_frame\n",
    "# detections = event.instance_detections2D\n",
    "\n",
    "# for key in detections.keys():\n",
    "#     print(str(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自动执行 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "\n",
    "test_name = \"sub_ep_test_0804_1_split1\"\n",
    "rec_rootpath = f\"./test_records/{test_name}/\"\n",
    "temp_result_path = f\"./exp_temp_result/{test_name}.txt\"\n",
    "\n",
    "def make_dir_by_filepath(filepath):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(filepath))\n",
    "    except OSError as exc: # 如果在创建目录时发生错误\n",
    "        if exc.errno != errno.EEXIST: # 如果错误不是“目录已经存在”的错误\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test_records/sub_ep_test_0804_1_split1/\n"
     ]
    }
   ],
   "source": [
    "start_index = 0\n",
    "length = 450\n",
    "print(rec_rootpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.init_epsoide(test_episodes[0])\n",
    "gpt_success = 0\n",
    "true_positive_success = 0\n",
    "rule_success = 0\n",
    "exceptioon_occr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(start_index, length):\n",
    "    ep = test_episodes[i]\n",
    "    # 修改日志文件目录\n",
    "    try:\n",
    "        rec_path = os.path.join(rec_rootpath, f\"{i}_{ep['id']}\")\n",
    "        agent.log_file_path = os.path.join(rec_path, \"gpt_chat_log.txt\")\n",
    "        agent.record_file_path = os.path.join(rec_path, \"trajectory_record_log.json\")\n",
    "        if not os.path.exists(os.path.dirname(agent.log_file_path)):\n",
    "            make_dir_by_filepath(agent.log_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"<Set log file path failed! Using default filepath. Detail: {e}>\")\n",
    "        agent.log_file_path = os.path.join(rec_rootpath, \"defalut_gpt_chat_log.txt\")\n",
    "        agent.record_file_path = os.path.join(rec_rootpath, \"defalut_trajectory_record_log.json\")\n",
    "        \n",
    "    # 自动执行\n",
    "    result = agent.auto_do_nav_episode(ep)\n",
    "    if result[2]:\n",
    "        gpt_success += result[0]\n",
    "        rule_success += result[1]\n",
    "        if result[0] == 1 and result[1] == 1:\n",
    "            true_positive_success += 1\n",
    "    else:\n",
    "        exceptioon_occr += 1\n",
    "    \n",
    "    res = {\n",
    "        \"Finished ep index\": i,\n",
    "        \"SR\": true_positive_success,\n",
    "        \"SR_GPT\" : gpt_success,\n",
    "        \"SR_RULE\" : rule_success,\n",
    "        \"Exception cnt\" : exceptioon_occr\n",
    "    }\n",
    "    # 写入临时结果\n",
    "    with open(temp_result_path, 'w') as f:\n",
    "        f.write(json.dumps(res))\n",
    "\n",
    "print(f\"------------------RESULT------------------\" )\n",
    "print(f\"SR = {true_positive_success / length}\" )\n",
    "print(f\"SR_GPT = {gpt_success / length}\" )\n",
    "print(f\"SR_RULE = {rule_success / length}\" )\n",
    "print(f\"exception cnt = {exceptioon_occr}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_done_in_second_last_line(dir_path):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if file == 'gpt_chat_log.txt':\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    if len(lines) >= 2 and 'Done' in lines[-2]:\n",
    "                        count += 1\n",
    "            \n",
    "    return count\n",
    "\n",
    "path = rec_rootpath\n",
    "print(f'Done: {count_done_in_second_last_line(path)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def count_done_and_rules_achieved(dir_path):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        if 'gpt_chat_log.txt' in files and 'trajectory_record_log.txt' in files:\n",
    "            gpt_log_path = os.path.join(root, 'gpt_chat_log.txt')\n",
    "            trajectory_log_path = os.path.join(root, 'trajectory_record_log.txt')\n",
    "\n",
    "            with open(gpt_log_path, 'r') as gpt_log:\n",
    "                lines = gpt_log.readlines()\n",
    "                gpt_condition = len(lines) >= 2 and 'Done' in lines[-2]\n",
    "\n",
    "            with open(trajectory_log_path, 'r') as trajectory_log:\n",
    "                json_list = json.load(trajectory_log)\n",
    "                if json_list:\n",
    "                    # print(json_list[-1])\n",
    "                    trajectory_condition = json_list[-1]['rules_achieve']\n",
    "            \n",
    "            if gpt_condition and not trajectory_condition:\n",
    "                print(gpt_log_path)\n",
    "\n",
    "            if gpt_condition and trajectory_condition:\n",
    "                count += 1\n",
    "                \n",
    "    return count\n",
    "print(f'True Positive: {count_done_and_rules_achieved(path)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def count_rules_achieved(dir_path):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        if 'trajectory_record_log.txt' in files:\n",
    "            gpt_log_path = os.path.join(root, 'gpt_chat_log.txt')\n",
    "            trajectory_log_path = os.path.join(root, 'trajectory_record_log.txt')\n",
    "\n",
    "            trajectory_condition = False\n",
    "            with open(trajectory_log_path, 'r') as trajectory_log:\n",
    "                json_list = json.load(trajectory_log)\n",
    "                if json_list:\n",
    "                    for each in json_list:\n",
    "                        if each['rules_achieve']:\n",
    "                            trajectory_condition = True\n",
    "                    # print(json_list[-1])\n",
    "                    # trajectory_condition = json_list[-1]['rules_achieve']\n",
    "\n",
    "            if trajectory_condition:\n",
    "                count += 1\n",
    "                \n",
    "    return count\n",
    "print(f'Positive: {count_rules_achieved(path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parsing output string\n",
    "import re\n",
    "msg_string = \"\"\"Thought [1]: The target object, GarbageCan, is not visible in the current observation. The maximum distance of accessibility is in the Rear direction with a distance of 6.50m. However, there are many objects in that direction, and the GarbageCan is not among them. The Rear Left direction has a maximum distance of accessibility of 1.77m, and there are also many objects in that direction, but again, the GarbageCan is not among them. The other directions have limited accessibility. Therefore, the best course of action would be to move towards the Rear direction, which has the highest accessibility and a variety of objects, increasing the chances of finding the GarbageCan.\n",
    "\n",
    "Action [1]: MoveAhead 6.50\n",
    "\n",
    "Observation [2]:\n",
    "<Direction: Front\n",
    "Objects detail: [{\"RemoteControl\": \"Middle center, 0.67m\"}, {\"Pot\": \"Middle right, 2.88m\"}, {\"Chair\": \"Middle center and Down center, 0.67m\"}, {\"Desk\": \"Middle center, 0.66m\"}, {\"Painting\": \"Middle center and Top center, 1.24m\"}, {\"ArmChair\": \"Middle right and Down right, 5.12m\"}, {\"Floor\": \"Middle center and Down center, 2.28m\"}, {\"Bed\": \"Middle right, 0.71m\"}, {\"BaseballBat\": \"Down center, 1.69m\"}]\n",
    "Maximum distance of accessibility: \"0.50m\"\"\"\n",
    "\n",
    "raw_msgs = msg_string.split('\\n')\n",
    "thought_str = \"\"\n",
    "action_str = \"\"\n",
    "find_flag = False\n",
    "for s in raw_msgs:\n",
    "    pattern_t = r'^Thought\\s*\\[(\\d+)\\]:\\s*(.*)$'\n",
    "    match = re.match(pattern_t, s)\n",
    "    if match:\n",
    "        thought_str = match.group(2)\n",
    "    else:\n",
    "        pattern_a = r'^Action\\s*\\[(\\d+)\\]:\\s*(.*)$'\n",
    "        match = re.match(pattern_a, s)\n",
    "        if match:\n",
    "            action_str = match.group(2)\n",
    "            find_flag = True\n",
    "            break\n",
    "if not find_flag:\n",
    "    print(\"[Operation]Error: receiving action output string with wrong format.\")\n",
    "else:\n",
    "    print(f\"T: {thought_str} \\nA: {action_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parsing output string\n",
    "import re\n",
    "action_str = \"\"\"Done\"\"\"\n",
    "action_pattern = r\"\\s*(MoveTo|RotateTo)\\s*(.*)\\s*|^(Done)$\"\n",
    "action_match = re.match(action_pattern, action_str)\n",
    "if action_match:\n",
    "    if action_match.group(3):\n",
    "        print(action_match.group(3) + \"?\")\n",
    "        res_dict = {\n",
    "            'name': action_match.group(3),\n",
    "            'param': \"\",\n",
    "            'action_type': OpType.FOLLOW_OBJ\n",
    "        }\n",
    "    else:\n",
    "        action_name = action_match.group(1)\n",
    "        print(action_match.group(2))\n",
    "        param1 = action_match.group(2)\n",
    "        res_dict = {\n",
    "            'name': action_name,\n",
    "            'param': param1,\n",
    "            'action_type': OpType.FOLLOW_OBJ\n",
    "        }\n",
    "    print(res_dict)\n",
    "else:\n",
    "    print(\"[Output Extract]The input string does not match the ACTION pattern.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
